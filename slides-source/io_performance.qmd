---
title: "I/O performance"
author: "Geert Jan Bex"
institution: "Vlaams Supercomputer Centrum"
date: today
format:
  revealjs:
    transiton: slide
    slide-number: true
code-annotations: select
---

## Overview

- Motivation
- Types of data & solutions
  - Tabular data
  - Text data
  - Multidimensional arrays
  - Images
- Applications & environments
- Wrap up


# Motivation


## Why worry?

- Input/Output may take time
- Large dataset, e.g., machine learning, data science
- File systems have characteristics

. . .

HPC file systems tuned for

- reading large files/large chunks
- writing large files/large chunks


## Many small read/writes

Metadata updates

- access time
- modification time
- file size

::: {.callout-warning .fragment}
Major performance degradation for... [entire system/all users!]{.fragment}
:::

::: {.callout-tip .fragment}
Use file systems well!
:::


# Types of data & solutions

Many data types

::: {.incremental}
- Tabular data: `.csv`, ...
- Text data: `.txt`, `.json`, ...
- Image data: `.jpeg`, `.tiff`, ...
- Video data: `.mp4`, ...
:::

. . .

Many small files [== many metadata operations]{.fragment}


## Solutions

::: {.incremental}
- Use binary file formats
- Bundle many small files into large one
- Read/Write multiple items at once
- Use caching
- Use node-local scratch space
:::


## Tabular data

Reading CSV is... [very slow]{.fragment}

. . .

Alternatives:
::: {.incremental}
- Parquet files
- Arrow files
:::

. . .

Can be read by pandas/polars/R


## Parquet versus Arrow

Parquet

- I/O & space efficient
- More CPU intensive for I/O (de)compression
- Not optimal for random access

. . .

Arrow

- I/O efficient
- In-memory format
- Random access okay, sequential operations very efficient


## Many text files

::: {.incremental}
- Create binary file & index
- Use Arrow
- Use TAR, ZIP
:::

::: {.callout-tip .fragment}
Read/write multiple "files" per operation
:::


## Multidimensional arrays

::: {.incremental}
- Use HDF5
- Use Pytorch/TensorFlow tensors in Arrow file
- Use binary files
:::

::: {.callout-warning .fragment}
Don't use ASCII files!
:::


## HDF5

Advantages

  - Programming language agnostic
  - Cross-platform
  - Self-documenting
  - Multiple datasets
  - Parallel I/O

. . .

Mostly for numerical data


## Image data

Many formats: JPEG, GIF, PNG, TIFF

. . .

Can have

- 1 channel: grayscale $\mapsto$ 2D array
- 3 channels: RGB or BGR $\mapsto$ 3D array
- 4 channels: RGB or BGR + alpha $\mapsto$ 3D array

. . .

Store as

- HDF5 file: dataset 3D array of `uint8`
- Arrow file: Pytorch or TensorFlow tensors


## Arrow files

How? [Use [Hugging Face ðŸ¤— datasets library](https://huggingface.co/docs/datasets/en/index)]{.fragment} 

::: {.incremental}
- Python only
- Data + metadata
- Easily split datasets for training & test
- Directory with Arrow file and JSON metadata
- Easy to use with Pytorch/TensorFlow
:::



# Applications & environments

## The problem with Python/R

Running script loads lots of modules/packages [== lots of metadata operations]{.fragment}

. . .

Solution: use containers

. . .

[Apptainer](https://apptainer.org/) is supported on VSC-systems


## Best practices & caveats

::: {.incremental}
- Build containers on target system
- Compile critical software components
- Be weary of `.local` et al.
- Use [hpccm](https://github.com/NVIDIA/hpc-container-maker)
:::


# Wrap up

## Conclusions

- Understaning I/O constraints is crucial
- Adapting workflows is (relatively) easy


## How to proceed?

If necessary, take trainings

- Linux intro
- HPC intro
- Python for HPC
- Parallel programming with MPI

. . .

Contact support
